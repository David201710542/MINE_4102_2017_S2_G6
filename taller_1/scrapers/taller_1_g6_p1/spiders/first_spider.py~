import scrapy
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor
from scrapy.exceptions import CloseSpider
###from taller_1.scrapers.taller_1_g6_p1.items import Taller1G6P1Item
import re
from taller_1_g6_p1.items import Taller1G6P1Item

class Taller1G6P1(CrawlSpider):
	name = 'Taller1G6P1'
	item_count = 0
	limite_paginas = 100
	url_base = 'https://uniandes.edu.co'
	allowed_domains = ['uniandes.edu.co']
#	start_urls = ['https://www.uniandes.edu.co/']
	start_urls = ['https://uniandes.edu.co/es/programas-facultades/lista-facultades/']

	rules = (
#		Rule(
#			LinkExtractor(
#				allow = (
#					re.compile(r'facultades', re.IGNORECASE),
#					re.compile(r'departamentos', re.IGNORECASE),
#					re.compile(r'programas', re.IGNORECASE)
#				), unique = True
#			), callback = 'filtrar_facultades', follow = True
#		),
		Rule(
			LinkExtractor(
				allow = (),
				restrict_xpaths = (
					'//div[contains(@class, "view-vista-lista-departamentos")]//ul/li[contains(@class, "views-row")]//a'
				), unique = True
			), callback = 'filtrar_noticias', follow = True
		),
#		Rule(
#			LinkExtractor(
#				allow = (),
#				restrict_xpaths = (
#					'//div[contains(@class, "menu-ppal")]//ul[contains(@class, "dropdown-menu")]'
#				), unique = True
#			), callback = 'menu_ppal', follow = True
#		)
	)

	def menu_ppal(self, response):
		for pag in response.xpath('//li[contains(@class, "leaf")]'):
			self.item_count += 1
			if self.item_count > self.limite_paginas:
				raise CloseSpider('demasiadas_paginas')
			facultad = Taller1G6P1Item()
			facultad['facultad'] = '(menu_ppal)' + pag.xpath('./a/text()').extract_first().strip()
			facultad['url'] = self.url_base + pag.xpath('.//a/@href').extract_first() if pag.xpath('.//a/@href').extract_first()[:1] == '/' else pag.xpath('.//a/@href').extract_first()
			facultad['fuente'] = ''
			yield facultad

	def filtrar_facultades(self, response):
		for pag in response.xpath('//div[contains(@class, "view-vista-lista-departamentos")]//ul/li[contains(@class, "views-row")]'):
			self.item_count += 1
			if self.item_count > self.limite_paginas:
				raise CloseSpider('demasiadas_paginas')
			facultad = Taller1G6P1Item()
			facultad['facultad'] = '(filtrar_facultades)' + pag.xpath('.//span[@class="field-content"]/text()').extract_first().strip()
			facultad['url'] = self.url_base + pag.xpath('.//a/@href').extract_first() if pag.xpath('.//a/@href').extract_first()[:1] == '/' else pag.xpath('.//a/@href').extract_first()
			facultad['fuente'] = response.url
			yield facultad

	def filtrar_noticias(self, response):
		test = Taller1G6P1Item()
		test['facultad'] = 'estoy dentro'		
		yield test
		for pag in response.xpath('//div[contains(@class, "noticia-list")]//a[contains(@class, "item-nl")]'):
			self.item_count += 1
			if self.item_count > self.limite_paginas:
				raise CloseSpider('demasiadas_paginas')
			facultad = Taller1G6P1Item()
			facultad['facultad'] = '(filtrar_noticias)' + pag.xpath('.//div[contains(@class, "date-nl")]/p/text/()').extract_first().strip()
			facultad['url'] = ''
			facultad['fuente'] = response.url
			yield facultad

#//div[contains(@class, "noticia-list")]//div[contains(@class, "info-nl")]/h2/@text
#//div[contains(@class, "noticia-list")]//div[contains(@class, "info-nl")]/div/@text
